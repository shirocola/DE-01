# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OKDAaQw3rDeX5PpFlBWJKJBR7n5qgJxw

"""

!apt-get update                                                                          # อัพเดท Package ทั้งหมด
!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                  # ติดตั้ง Java Development Kit
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz # ติดตั้ง Spark 3.1.2
!tar xzvf spark-3.1.2-bin-hadoop2.7.tgz                                                  # Unzip ไฟล์ Spark 3.1.2
!pip install -q findspark==1.3.0                                                         # ติดตั้ง Package Python สำหรับเชื่อมต่อกับ Spark

# Set enviroment variable
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

# install pyspark
!pip install pyspark==3.1.2

"""## ใช้งาน Spark"""

# Create Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

# ดูเวอร์ชั่น Python
import sys
sys.version_info

# ดูเวอร์ชั่น Spark
spark.version

"""## Load Data"""

# Download Data File
!wget -O data.zip https://file.designil.com/zdOfUE+
!unzip data.zip

dt = spark.read.csv('/content/ws2_data.csv', header = True, inferSchema = True, )

"""# Data Profiling"""

# ดูว่ามีคอลัมน์อะไรบ้าง
dt

# ดูข้อมูล 10 แถวแรก
dt.show(10)

# ดูประเภทข้อมูล
dt.dtypes

# นับจำนวนแถวและ column
print((dt.count(), len(dt.columns)))

# สรุปข้อมูลสถิติ (เช่น ค่าเฉลี่ย ค่ามากสุด ค่าน้อยสุด)
dt.describe().show()

# สรุปข้อมูลสถิติเฉพาะ column ที่เลือก
dt.select("price").describe().show()

# แสดงแถวที่มีค่า Missing Value
dt.where( dt.user_id.isNull() ).show()

"""# EDA - Exploratory Data Analysis

## Non-Graphical EDA

เราสามารถใช้คำสั่ง Spark ในการค้นหาข้อมูลที่ต้องการได้
"""

# ข้อมูลที่เป็นตัวเลข
dt.where(dt.price >= 1).show()

# ข้อมูลที่เป็นตัวหนังสือ
dt.where(dt.country == 'Canada').show()

# การซื้อทั้งหมดที่เกิดขึ้นในเดือนพฤษภาคม
dt.where( dt.timestamp.startswith("2021-05") ).count()

"""## Graphical EDA


Import seaborn for visualisation
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# แปลง Spark Dataframe เป็น Pandas Dataframe
dt_pd = dt.toPandas()

# ดูตัวอย่างข้อมูล
dt_pd.head()

# Boxplot
sns.boxplot(x = dt_pd['book_id'])

# Histogram
# bins = จำนวน bar ที่ต้องการแสดง
sns.histplot(dt_pd['price'], bins=15)

# Scatterplot - แสดงความสัมพันธ์ระหว่างคอลัมน์
sns.scatterplot(x=dt_pd.book_id, y=dt_pd.price)

# Plotly - interactive chart
import plotly.express as px
fig = px.scatter(dt_pd, 'book_id', 'price')
fig.show()

"""# Data Cleansing with Spark"""

# แปลง string เป็น datetime
from pyspark.sql import functions as f

dt_clean = dt.withColumn("timestamp",
                        f.to_timestamp(dt.timestamp, 'yyyy-MM-dd HH:mm:ss')
                        )
dt_clean.show()

dt_clean.printSchema()

"""## Anomalies Check

### เช็คการสะกด
"""

dt_clean.select("Country").distinct().count()

dt_clean.select("Country").distinct().sort("Country").show( 58, False )

dt_clean.where(dt_clean['Country'] == 'Japane').show()

# เปลี่ยน Japane เป็น Japan
from pyspark.sql.functions import when

dt_clean_country = dt_clean.withColumn("CountryUpdate", when(dt_clean['Country'] == 'Japane', 'Japan').otherwise(dt_clean['Country']))

# ตรวจสอบข้อมูลที่แก้ไขแล้ว
dt_clean_country.select("CountryUpdate").distinct().sort("CountryUpdate").show(58, False)

# ดูหน้าตาข้อมูลตอนนี้
dt_clean_country.show()

# เอาคอลัมน์ CountryUpdate ไปแทนที่คอลัมน์ Country
dt_clean = dt_clean_country.drop("Country").withColumnRenamed('CountryUpdate', 'Country')

# ดูหน้าตาข้อมูล
dt_clean.show()

"""### เช็คค่าที่อยู่นอกเหนือขอบเขต
"""

# ดูข้อมูล user_id 
dt_clean.select("user_id").show(10)

# นับจำนวน user_id ทั้งหมด
dt_clean.select("user_id").count()

# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ
dt_clean.where(dt_clean["user_id"].rlike("^[a-z0-9]{8}$")).count()

# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ
dt_correct_userid = dt_clean.filter(dt_clean["user_id"].rlike("^[a-z0-9]{8}$"))
dt_incorrect_userid = dt_clean.subtract(dt_correct_userid)

dt_incorrect_userid.show(10)

dt_clean_userid = dt_clean.withColumn("user_id_update", when(dt_clean['user_id'] == 'ca86d17200', 'ca86d172').otherwise(dt_clean['user_id']))

# ตรวจสอบผลลัพธ์
dt_correct_userid = dt_clean_userid.filter(dt_clean_userid["user_id"].rlike("^[a-z0-9]{8}$"))
dt_incorrect_userid = dt_clean_userid.subtract(dt_correct_userid)

dt_incorrect_userid.show(10)

# ดรอปคอลัมน์ user_id เปลี่ยนชื่อ user_id_update เป็น user_id
dt_clean = dt_clean_userid.drop("user_id").withColumnRenamed('user_id_update', 'user_id')

"""### Missing values คือ ค่าที่ขาดหายไปจาก data"""

# เช็ค Missing Value

from pyspark.sql.functions import col, sum

dt_nulllist = dt_clean.select([ sum(col(colname).isNull().cast("int")).alias(colname) for colname in dt_clean.columns ])
dt_nulllist.show()

# ดูช้อมูลว่าแถวไหนมี user_id เป็นค่าว่างเปล่า

dt_clean.where( dt.user_id.isNull() ).show()

# แทน user_id ที่เป็น NULL ด้วย 00000000
dt_clean_userid = dt_clean.withColumn("user_id_update", when(dt_clean['user_id'].isNull(), '00000000').otherwise(dt_clean['user_id']))
dt_clean_userid.show(10)

dt_clean = dt_clean_userid.drop("user_id").withColumnRenamed('user_id_update', 'user_id')
dt_clean

# เช็คว่า user ID ที่เป็น NULL หายไปแล้วจริงมั้ย
dt_clean.where( dt_clean.user_id.isNull() ).show()

"""###  Outliers:ค่าที่มากผิดปกติ """

dt_clean_pd = dt_clean.toPandas()

sns.boxplot(x = dt_clean_pd['price'])

dt_clean.where( dt_clean.price > 80 ).select("book_id").distinct().show()

"""# Save data เป็น CSV

โดยปกติแล้ว Spark จะทำการ Save แบบหลายไฟล์ เพราะปกติใช้หลายเครื่องในการประมวลผล
"""

# เซฟแบบแบ่งไฟล์
dt_clean.write.csv('Cleaned_data.csv', header = True)

# เซฟเป็นไฟล์เดียว
dt_clean.coalesce(1).write.csv('Cleaned_Data_Single.csv', header = True)

"""##วิธีอ่านไฟล์ที่มีหลาย Part

"""

all_parts = spark.read.csv('/content/Cleaned_data.csv/part-*.csv', header = True, inferSchema = True)

all_parts.count()
