# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OKDAaQw3rDeX5PpFlBWJKJBR7n5qgJxw

#ติดตั้ง Spark และ PySpark
"""

!apt-get update                                                                          # อัพเดท Package ทั้งหมดใน VM ตัวนี้
!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                  # ติดตั้ง Java Development Kit (จำเป็นสำหรับการติดตั้ง Spark)
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz # ติดตั้ง Spark 3.1.2
!tar xzvf spark-3.1.2-bin-hadoop2.7.tgz                                                  # Unzip ไฟล์ Spark 3.1.2
!pip install -q findspark==1.3.0                                                         # ติดตั้ง Package Python สำหรับเชื่อมต่อกับ Spark

# Set enviroment variable ให้ Python รู้จัก Spark
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

# ติดตั้ง PySpark ลงใน Python
!pip install pyspark==3.1.2

"""## ใช้งาน Spark"""

# สร้าง Spark Session เพิ้อใช้งาน Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()

# ดูเวอร์ชั่น Python
import sys
sys.version_info

# ดูเวอร์ชั่น Spark
spark.version

"""## Load Data"""

# Download Data File
!wget -O data.zip https://file.designil.com/zdOfUE+
!unzip data.zip

dt = spark.read.csv('/content/ws2_data.csv', header = True, inferSchema = True, )

"""# Data Profiling"""

# ดูว่ามีคอลัมน์อะไรบ้าง
dt

# ดูข้อมูล 20 แถวแรก
dt.show(20)

# ดูประเภทข้อมูลแต่ละคอลัมน์
dt.dtypes

# นับจำนวนแถวและ column
print((dt.count(), len(dt.columns)))

# สรุปข้อมูลสถิติ
dt.describe().show()

# สรุปข้อมูลสถิติเฉพาะ column ที่ระบุ
dt.select("price").describe().show()

# แสดงแถวข้อมูลที่มี Missing Value
dt.where( dt.user_id.isNull() ).show()

"""# EDA - Exploratory Data Analysis

## Non-Graphical EDA

เราสามารถใช้คำสั่ง Spark ในการค้นหาข้อมูลที่ต้องการได้
"""

# ข้อมูลที่เป็นตัวเลข
dt.where(dt.price >= 1).show()

# ข้อมูลที่เป็นตัวหนังสือ
dt.where(dt.country == 'Canada').show()

# การซื้อทั้งหมดที่เกิดขึ้นในเดือนพฤษภาคม
dt.where( dt.timestamp.startswith("2021-05") ).count()

"""## Graphical EDA


Spark ไม่ได้ถูกพัฒนามาเพื่องาน plot ข้อมูล เพราะฉะนั้นเราจะใช้ package `seaborn` `matplotlib` และ `pandas` ในการ plot ข้อมูลแทน
"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# แปลง Spark Dataframe เป็น Pandas Dataframe - ใช้เวลาประมาณ 6 วินาที
dt_pd = dt.toPandas()

# ดูตัวอย่างข้อมูล
dt_pd.head()

# Boxplot - แสดงการกระจายตัวของข้อมูลตัวเลข
sns.boxplot(x = dt_pd['book_id'])

# Histogram - แสดงการกระจายตัวของข้อมูลตัวเลข
# bins = จำนวน bar ที่ต้องการแสดง
sns.histplot(dt_pd['price'], bins=15)

# Scatterplot - แสดงความสัมพันธ์ระหว่างคอลัมน์
sns.scatterplot(x=dt_pd.book_id, y=dt_pd.price)

# Plotly - interactive chart
import plotly.express as px
fig = px.scatter(dt_pd, 'book_id', 'price')
fig.show()

"""# Data Cleansing with Spark"""

# แปลง string เป็น datetime
from pyspark.sql import functions as f

dt_clean = dt.withColumn("timestamp",
                        f.to_timestamp(dt.timestamp, 'yyyy-MM-dd HH:mm:ss')
                        )
dt_clean.show()

dt_clean.printSchema()

"""## Anomalies Check

### ความผิดปกติ 1) Syntactical Anomalies
**Lexical errors** เช่น สะกดผิด
"""

dt_clean.select("Country").distinct().count()

dt_clean.select("Country").distinct().sort("Country").show( 58, False )

dt_clean.where(dt_clean['Country'] == 'Japane').show()

# เปลี่ยน Japane เป็น Japan
from pyspark.sql.functions import when

dt_clean_country = dt_clean.withColumn("CountryUpdate", when(dt_clean['Country'] == 'Japane', 'Japan').otherwise(dt_clean['Country']))

# ตรวจสอบข้อมูลที่แก้ไขแล้ว
dt_clean_country.select("CountryUpdate").distinct().sort("CountryUpdate").show(58, False)

# ดูหน้าตาข้อมูลตอนนี้
dt_clean_country.show()

# เอาคอลัมน์ CountryUpdate ไปแทนที่คอลัมน์ Country
dt_clean = dt_clean_country.drop("Country").withColumnRenamed('CountryUpdate', 'Country')

# ดูหน้าตาข้อมูล
dt_clean.show()

"""### ความผิดปกติ 2) Semantic Anomalies

**Integrity constraints**: ค่าอยู่นอกเหนือขอบเขตของค่าที่รับได้ เช่น
- user_id: ค่าจะต้องเป็นตัวเลขหรือตัวหนังสือ 8 ตัวอักษร
"""

# ดูว่าข้อมูล user_id ตอนนี้หน้าตาเป็นอย่างไร
dt_clean.select("user_id").show(10)

# นับจำนวน user_id ทั้งหมด
dt_clean.select("user_id").count()

# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ
dt_clean.where(dt_clean["user_id"].rlike("^[a-z0-9]{8}$")).count()

# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ
dt_correct_userid = dt_clean.filter(dt_clean["user_id"].rlike("^[a-z0-9]{8}$"))
dt_incorrect_userid = dt_clean.subtract(dt_correct_userid)

dt_incorrect_userid.show(10)

dt_clean_userid = dt_clean.withColumn("user_id_update", when(dt_clean['user_id'] == 'ca86d17200', 'ca86d172').otherwise(dt_clean['user_id']))

# ตรวจสอบผลลัพธ์
dt_correct_userid = dt_clean_userid.filter(dt_clean_userid["user_id"].rlike("^[a-z0-9]{8}$"))
dt_incorrect_userid = dt_clean_userid.subtract(dt_correct_userid)

dt_incorrect_userid.show(10)

# เอาคอลัมน์ user_id_update ไปแทนที่ user_id
dt_clean = dt_clean_userid.drop("user_id").withColumnRenamed('user_id_update', 'user_id')

"""### ความผิดปกติ 3) Missing values

การเช็คและแก้ไข Missing Values (หากจำเป็น)

ค่า Missing Value คือ ค่าที่ว่างเปล่า

เราจะรู้ได้ยังไงว่าคอลัมน์ไหนมีค่าว่างเปล่ากี่ค่า
"""

# เช็ค Missing Value

from pyspark.sql.functions import col, sum

dt_nulllist = dt_clean.select([ sum(col(colname).isNull().cast("int")).alias(colname) for colname in dt_clean.columns ])
dt_nulllist.show()

# ดูช้อมูลว่าแถวไหนมี user_id เป็นค่าว่างเปล่า

dt_clean.where( dt.user_id.isNull() ).show()

# แทน user_id ที่เป็น NULL ด้วย 00000000
dt_clean_userid = dt_clean.withColumn("user_id_update", when(dt_clean['user_id'].isNull(), '00000000').otherwise(dt_clean['user_id']))
dt_clean_userid.show(10)

dt_clean = dt_clean_userid.drop("user_id").withColumnRenamed('user_id_update', 'user_id')
dt_clean

# เช็คว่า user ID ที่เป็น NULL หายไปแล้วจริงมั้ย
dt_clean.where( dt_clean.user_id.isNull() ).show()

"""### ความผิดปกติ 4) Outliers:

ข้อมูลที่สูงหรือต่ำผิดปกติจากข้อมูลส่วนใหญ่

มาลองใช้ Boxplot ในการหาค่า Outlier ของราคาหนังสือ
"""

dt_clean_pd = dt_clean.toPandas()

sns.boxplot(x = dt_clean_pd['price'])

dt_clean.where( dt_clean.price > 80 ).select("book_id").distinct().show()

"""# Save data เป็น CSV

โดยปกติแล้ว Spark จะทำการ Save ออกมาเป็นหลายไฟล์ เพราะใช้หลายเครื่องในการประมวลผล
"""

# เซฟเป็น partitioned files (ใช้ multiple workers)
dt_clean.write.csv('Cleaned_data.csv', header = True)

# เซฟเป็น 1 ไฟล์ (ใช้ single worker)
dt_clean.coalesce(1).write.csv('Cleaned_Data_Single.csv', header = True)

"""##วิธีอ่านไฟล์ที่มีหลาย Part

"""

all_parts = spark.read.csv('/content/Cleaned_data.csv/part-*.csv', header = True, inferSchema = True)

all_parts.count()